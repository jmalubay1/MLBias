{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "import os\r\n",
                "import numpy as np\r\n",
                "import tensorflow as tf \r\n",
                "from tensorflow import keras \r\n",
                "from keras import layers, models\r\n",
                "from keras.models import Model\r\n",
                "import pandas as pd\r\n",
                "import matplotlib.pyplot as plt\r\n",
                "from sklearn import preprocessing\r\n",
                "from sklearn.metrics import mean_absolute_error\r\n",
                "from sklearn.metrics import mean_squared_error\r\n",
                "from sklearn.model_selection import train_test_split\r\n",
                "\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# DATA PREPROCESSING\r\n",
                "# load data\r\n",
                "DATA_PATH = 'c:/Users/johns/Desktop/propublica/csv/compas-scores-two-years.csv'\r\n",
                "data = pd.read_csv(DATA_PATH)\r\n",
                "\r\n",
                "# isolate decile score for truth labels\r\n",
                "truth_labels = data[['decile_score']]\r\n",
                "\r\n",
                "# drop unwanted dataframes (like redudant ones such as dates)\r\n",
                "data = data[['sex', 'age_cat', 'race', 'juv_fel_count',\r\n",
                "              'juv_misd_count', 'juv_other_count', 'priors_count',\r\n",
                "              'days_b_screening_arrest', 'c_days_from_compas',\r\n",
                "              'c_charge_degree', 'is_recid', 'r_charge_degree', 'r_days_from_arrest']]\r\n",
                "\r\n",
                "# 1-hot encode categorical data like race, sex etc\r\n",
                "# list object type columns\r\n",
                "cols = []\r\n",
                "for i in range(len(data.columns)):\r\n",
                "    col = data.iloc[:, i]\r\n",
                "    if col.dtype == 'object':\r\n",
                "        cols.append(col.name)\r\n",
                "\r\n",
                "# replace categorical columns with 1 hot encoding columns for each option\r\n",
                "one_hot_data = pd.get_dummies(data, columns=cols)\r\n",
                "\r\n",
                "n_att = len(one_hot_data.columns)\r\n",
                "\r\n",
                "# convert to numpy arrays\r\n",
                "truth_labels = truth_labels.to_numpy()\r\n",
                "one_hot_data = one_hot_data.to_numpy()\r\n",
                "\r\n",
                "# shuffle and split data into training/validation sets 80:20 ratio *before normalization* \r\n",
                "x_train, x_val, y_train, y_val = train_test_split(one_hot_data, truth_labels, train_size=0.80)\r\n",
                "\r\n",
                "# scale data\r\n",
                "y_train=np.reshape(y_train, (-1,1))\r\n",
                "y_val=np.reshape(y_val, (-1,1))\r\n",
                "scaler_x = preprocessing.MinMaxScaler()\r\n",
                "scaler_y = preprocessing.MinMaxScaler()\r\n",
                "scaler_x.fit(x_train)\r\n",
                "xtrain_scale=scaler_x.transform(x_train)\r\n",
                "scaler_x.fit(x_val)\r\n",
                "xval_scale=scaler_x.transform(x_val)\r\n",
                "scaler_y.fit(y_train)\r\n",
                "ytrain_scale=scaler_y.transform(y_train)\r\n",
                "scaler_y.fit(y_val)\r\n",
                "yval_scale=scaler_y.transform(y_val)\r\n",
                "\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# MODEL IMPLEMENTATION\r\n",
                "\r\n",
                "# DNN generally only have a single hidden layer, as far as n_nuerons usually you want some\r\n",
                "# number between the input and output sizes, one rule of thumb is n_samples / (a *(n_in + n_out))\r\n",
                "# where 2 <= a <= 10, gunna use that and play with it as needed, 16's (for n_in=32, n_out=1) a good \r\n",
                "# starting point. Also think treating this as a regression problem makes sense since the decile \r\n",
                "# scores are not categorical and are very much a scale. (originally were thinking softmax so output\r\n",
                "# activation is now linear)\r\n",
                "n_batch = 32\r\n",
                "n_epochs = int(len(x_train) / n_batch)\r\n",
                "\r\n",
                "# define model \r\n",
                "model = models.Sequential()\r\n",
                "model.add(layers.Dense(n_att, input_dim=n_att, kernel_initializer='normal', activation='relu'))\r\n",
                "model.add(layers.Dense(16, activation='relu'))\r\n",
                "model.add(layers.Dense(1, activation='linear'))\r\n",
                "#model.add(layers.Dense(1, kernel_initializer='normal'))\r\n",
                "\r\n",
                "#model.summary()\r\n",
                "\r\n",
                "model.compile(loss='mse', optimizer='adam', metrics=['mse', 'mae'])\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# MODEL TRAINING 1\r\n",
                "\r\n",
                "# train model\r\n",
                "# starting with e50 b32 as a jumping off point\r\n",
                "#history = model.fit(x_train, y_train, epochs=50, batch_size=32, verbose=1, validation_split=0.2)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# MODEL TRAINING 2\r\n",
                "\r\n",
                "# havent used keras regressor before but having some kind of trouble with it\r\n",
                "\r\n",
                "# evaluate model\r\n",
                "from keras.wrappers.scikit_learn import KerasRegressor\r\n",
                "from sklearn.model_selection import cross_val_score\r\n",
                "from sklearn.model_selection import KFold\r\n",
                "\r\n",
                "estimator = KerasRegressor(build_fn=model, epochs=n_epochs, batch_size=n_batch, verbose=0)\r\n",
                "kfold = KFold(n_splits=10)\r\n",
                "results = cross_val_score(estimator, x_train, y_train, cv=kfold)\r\n",
                "print(\"model mean and std_dev: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
            ],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.8.5",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.8.5 64-bit"
        },
        "interpreter": {
            "hash": "5ee19cf5417b771657d090c90a22d72bc0cae4115c1bcb524407b99a6b053f75"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}